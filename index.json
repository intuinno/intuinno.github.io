[{"authors":["Deokgun Park"],"categories":null,"content":"I get many emails from master students who are interested in reserach.\n I would like to get research experience in my lab. May I join your lab as a master student?\n Below you can find my advice. TLDR, if you plan to pursue PhD after your master, it might be a good idea to experience research in my lab. If you plan to industry career, it might not.\nYou can contact me and discuss your interest. However, if you want to get a job after graduation, doing research with me might not be helpful. You might get a better chance for the job by preparing the coding interview. The research takes time to produce a meaningful outcome. And the research is a full-time job. For example, it is common that it takes more than two years before a phd student can produce a paper or meaningful software. However, this can be too late for the master students who need something to show to potential employer during first semester and third semester. Doing research is more beneficial for those who consider phd program after graduation.\nStill there are many masters student working in my lab. I encourage you to contact them.\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565105936,"objectID":"6e6fe2f39189ef68295dd2273703753e","permalink":"http://crystal.uta.edu/~park/post/master-research/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/master-research/","section":"post","summary":"If you plan to pursue PhD after your master, it might be a good idea to experience research in my lab. If you plan to industry career, it might not. ","tags":["student advice"],"title":"Advice for the master students who want to do reserach in my lab","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Recently, I got an email from a smart undergraduate student who was interested in research. I think his situation might be a common case so here I added my reply in the hope that this can be helpful to other students. Below is the modified email message.\n I have been talking with few graduate recruiters and found out that graduate schools don’t offer scholarship unless I am doing PhD or thesis (which I had no idea what that meant). All I wanted to do was to get a masters degree to gain enough knowledge on AI and deep learning and work for companies like DeepMind or Neuralink to bring AGI and ASI to real life. And now I am overwhelmed and lost with what I’m supposed to do. My options are to loan my way through graduate school (but I can’t do that since I am already on a huge debt for my undergraduate school), or get a PhD (this will take me forever and I will never be able to pay my loans back with the stipend), or find a job (until I am able to pay by loans and fund my graduate school but I really want to be on AI). I recently figured out that my major (computer engineering) wasn’t right for me. I decided to switch to computer science but I needed to take more classes so I am switching to software engineering to graduate on time with right the classes. My mind’s all over the place and I have no idea where to seek help from. I’m only halfway through the book even though I love it. I have been dying to learn Reinforcement Learning but I haven’t got time even to start searching about the resources.\nThank you so much, Dr. Park! I hope you have a wonderful rest of the day.\n Below you can find my advice. TLDR, it is okay to get a job if you are on a debt. If you think you are interested in the research, try to study on your own for a year or two. If you are still interested, you can consider a PhD.\nMy answer to the student Hi, Tomas (Name changed for privacy)\nI understand your position.\nBelow is my advice. The choice is yours. There is no right or wrong choice. The choice is based on who you are and in return shapes who you will be.\n to loan my way through graduate school (but I can’t do that since I am already on a huge debt for my undergraduate school),\n The graduate school here must be master’s degree, right. I don’t recommend it, either. You cannot get enough knowledge on AI topic and you don’t have enough time to create a knowledge that will make you attractable hire for companies like Deep Mind.\n or get a PhD (this will take me forever and I will never be able to pay my loans back with the stipend),\n It does not take you forever. It is actually somewhat short to do what I describe above. It will pay you back reasonably well if you are successful. But the decision cannot be made solely on financial reward because this financial reward is probabilistic. Better motivation would be the pursuit of knowledge or curiosity. And the reward is auxiliary. For example, you usually do not pursue professional baseball career because most of baseball related careers are not well-paid. Only winner takes it all.(I hope the chance is higher in the CS.) But many still pursue it because they enjoy baseball anyway.\n or find a job (until I am able to pay by loans and fund my graduate school but I really want to be on AI).\n This is an actually good option. Get a job. Get decent salary and pay your loans. The trick is that you can study AI on your own with online learning education resource nowadays. This can be sometimes better than what the Master’s degree can offer. When you study enough and you have a non-trivial question, it will be a good time to pursue PhD. Of course, consult with professors about your question.\nAttending academic conference is a good investment. Studying alone is actually harder than you might think. You will be busy for the job (actually everyone is busy, right? Even my 9th grade daughter is busy because she has to watch three hours of youtube videos in addition to all the school work and violin and archery practice) and you may find the AI study is not so fun. The good news is that if you don’t like it, you don’t have to do it. It means that you are probably not good fit for the PhD. Don’t worry. You have a job already and you can focus on the software engineering career which can be also very rewarding. According to my opinion, I think about 5% of the people with good school grades are actually good fit for the PhD. The main reason is that we get good grades by learning what others have found. But PhD is about finding new knowledge which is very different from learning what others have found.\nAs a summary, I think you have a potential. If you really want to study AI and deep learning now, you can get a PhD now. If you are not sure, it is okay to get a SWE job. If you find that you keep studying on AI and DL for one or two years on your own, you might consider getting a PhD.\nRegards\nDeokgun\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565105690,"objectID":"d836ea05e94048f86891c48151975cb2","permalink":"http://crystal.uta.edu/~park/post/graduate-school/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/graduate-school/","section":"post","summary":"If you are not sure, getting a SW job is okay. If you study AI by yourself for one or two years, you might consider applying for PhD program.","tags":["student advice"],"title":"Advice for the undergraduate students who consider the graduate school","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Recently, I got an email from a student from HDILab. I think his situation might be a common case so here I added my reply in the hope that this can be helpful to other students. Below is the modified email message from the student.\n Hello Dr.Park,\nI am trying to implement carracing. I am using a virtualenv python3.5.4. I am trying to run extract.bash using $sudo bash extract.bash. However, it returns\n” File “extract.py”, line 8, in  import gym ImportError: No module named gym” when extract.bash is trying to run extract.py.\nThe problem is I installed gym and box2d-py too, and yet it shows the import error.\n$ which pip /home/aishwarya/WorldModelsExperiments/carracing/venv/bin/pip\n$ which python\n/home/aishwarya/WorldModelsExperiments/carracing/venv/bin/python\nI’ve also tried the solutions given in here and in here\nCan you please help me out with this issue.\n Below you can find my advice. TLDR, if you have a technical problem. Solve it on your own while producing shareable outcome. If you have a research idea, your advisor can help you evaluate whether it is intractable, already done, or trivial.\nMy answer to the student Hi, Tomas (Name changed for privacy)\nI don\u0026rsquo;t know about this. You should check Google or ask other person\u0026rsquo;s in the lab. I think Jane and John might know.\nDon\u0026rsquo;t worry. Debugging this kind issue is common. I myself always deal with this issue. One of the reason, I cannot help is that the reason can be diverse. It is case by case. Therefore it is more important important to know how to deal with these kind of issues than the actual specific solution. There will be hardly anyone who can solve your technical problem with a clear answer. If you have one you are lucky. But still if you ask to much questions to him, you are actually costing a precious resource to the group which is a bad news for you. Below are some general advices.\n Use slack general channel. If you send email to one person, the chance is low that your busy and ignorant professor can help you. If you use slack to post your issue to the entire group, some competent students might help you.\n Make environment simple. 90 Percent of these issues are package version collision. Virtual env helps but not perfect. In old days I used to format frequently. That\u0026rsquo;s why I use one disk for system and one disk for home directory. Nowadays learning how to use docker will pay you back well in the long run.\n Try to learn generalizable lesson and share it. Most of time, you just fix and forget. You might try stack overflow or Google solution until you fix it. The problem with this approach is that the time you spent on solving this issue does not produce any credit for your work. While I also sometimes do that, the correct way to handle an issue is that first understand the reason of the issue and create a technical note or blogs that explains the issue and the solution. Many people in the lab can benefit your contribution and your technical blog will help building your professional reputation. You need to be accustomed to solve these kinds of issues. It is like growing your muscles.\n  How to use your advisor If your advisor cannot help you solve your technical issues, where can I use him? One area is when you have a research question. As a graduate student, you will always think the research question or the topic for the next paper. There can be two major pitfalls for setting the research topic. An academic work is about finding a knowledge that none has found before. There can be two reasons why nobody has done it before.\nFirst, it can be too difficult to do or trivial but not meaningful. When we think a new idea, I can bet there will be more than 10 people who have thought the same thing. Still the finding the solution might be too difficult. Time machine, teleportation, or thinking machines are those areas. Those are technologies that people wish but it is very hard to make a progress in those area. Or more commonly, your topic is slightly general and there can be many approaches to it. However, the topic might be well-known in the community and almost all low hanging fruits are taken already and what remains might be too challenging.\nSecond, your problem might be too narrow or trivial or already solved before. Your advisor is like a guide to the Mt. Everest or a chaperone to the party. He has an experience and background knowledge about the research community.\nTherefore getting a cross check of your idea can be a good use of your advisor.\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565102669,"objectID":"dbd9435e0f3601e2c69d958eac3fb883","permalink":"http://crystal.uta.edu/~park/post/how-to-use-your-advisor/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/how-to-use-your-advisor/","section":"post","summary":"If you have a technical problem. Solve it on your own while producing shareable outcome. If you have a research idea, your advisor can help you evaluate whether it is intractable, already done, or trivial. ","tags":["student advice"],"title":"How to use your advisor","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Below is HDILab culture. A culture is a principle about how we do everyday things. I hope that our culture shape our group in a productive way.\nWe learn by teaching. Every week we will have a paper reading seminar where one student will teach a paper to other lab members. A pizza and a diet coke will be provided. You will have a chance to announce what you are working on nowadays in a friendly environment 30 minutes before the seminar. Currently the seminar starts on Wednesday 12:30pm.\n After you have done lab seminar, add the slides and all materials into the Google drive here. The goal here is to make it easier for a new lab member to follow what we have done so far. Also write one paragraph abstract for the seminar to summarize what you teach. Add your call sign so that we can know who did the seminar. Students should answer the questions from colleagues.  Team play There are focused hours in the lab where students are supposed to be at the same space and time. The main idea is to have an informal chat, update, questions and answers. The focused hours are currently held Mon, Tue, Wed 10am-2pm at ERB 205. Don’t be late. If you think you will be late, report it to the slack.\nSharable output Students are required to maintain a lab journal and project journal Lab journal is for the general purpose work and project journal is for the project related progress. For example,\n I set up the new server or hardware for the-\u0026gt; hdilab journal I organized the inventory -\u0026gt; hdilab journal\n I replicated the world model -\u0026gt; daivid journal  Every work should be recorded and reproducible. Basically journal is for yourself. But write it well when others have to find something you did. When you accumulate enough work and progress, make them shareable and transferable chunk into three places:\n journal how-to technical note When you create a new technical note leave a author call sign such as #deokgun so that I can find who wrote what. If you need to update it, add a call sign and date after the original author.  Example Created: #deokgun 01/08/2019 Updated: #sanjay 01/09/2019   Personal homepage blog article Github readme.md  One common mistake for the new member is writing a technical note about a topic that is already covered in previous technical note from other members. This is problematic in the two sense. First, it means that you did not search or read other\u0026rsquo;s technical note. A value of the technical note is how many times it is read by others. If no one reads others\u0026rsquo; technical note, the value of our journal will be nothing. Second, you are creating an additional burden for the future readers looking for a solution for the topic. In that sense, you are creating a noise if you are creating a redundant article. So what we should do when we found that the topic that we would like to wrtie is available. Check the original post and if you have an additional content, just update the original post. If you are sure that you can write a better one by rewriting, replace the original post. The original post should be archived to the old technical note and referenced by the new article.\nWe share the open source code and how-to manual as a final output. We use GitHub as the main archive As a rule of thumb, I expect 50% of everyone’s time will be spent writing what you have done.\nHow to write a journal  Make the outline neat If you click the View-\u0026gt; Show Document Outline button, it will show you the document outline. Make sure your writing follows the header rule. There is a shortcut key for selecting the paragraph level.  cmd + alt + 1 makes the top level heading cmd + alt + 2 makes the second level heading cmd + alt + 0 makes the content paragraph, which is not shown in outline.   The Things to do next part is important. Your professor reads it often. Keep it tidy. When something is done, remove. Update it everyday. When you are given new goal, write it there. Make the thing you are working on now, the first item. Order them in the priority.  Communication is half of your job. Your impact will be the multiplication of research and communication of the result. Try to report the progress whenever there is a progress Don\u0026rsquo;t wait until your boss ask your progress. Tell him what you are working on now. If you meet a barrier or decide to change direction, report it. When I ask you to do something, I usually expect to hear the progress report in a one or two days.\nBeing true to ourselves You cannot deceive yourself. Being true to yourself is, in my opinion, a very good long-term strategy. For example, if you don’t know during the seminar, acknowledge that you don’t know and ask questions until you understand. Don’t be embarrassed or give up by social pressure. Academy is a few places where being true to yourself is tolerated.\nOther good question to ask yourself is whether you are enjoying the research. Actually according to my experience, research is not for everyone. Probably about 5 persons out of 100 suit the academic life. Because of this, it is perfectly okay if you feel that the graduate study is not what you want to do. I recommend to get a SW engineering job in that case, which actually pays you better and provides better work-life balance. You might feel like a failure to quit the research at first, but if you are true to yourself, you may find it as one of the best decisions of your life later.\nSimilarly I will release you out of the lab if I think that you are not a good fit for the lab as early as possible. How I make that decision? First, I have to acknowledge that it is very difficult decision for me, too. It is not only emotionally difficult but intellectually difficult. Because nobody knows the future, one day the student I fired may get a Nobel prize.\nI have to follow the lab culture which is my principle. For me, there are three characteristics for the academic career.\n First is curiosity. The desire to know or find the truth is our strongest motivation and rewards.\n Second is the self-starting. This one is related to curiosity. Because you want to know something, you start researching about it. This is usually observed when the student comes to me and ask an advice for something. It is the opposite of waiting for the order from me.\n Third is the persistence. The journey is long and the rewards are sparse. If you are not persistent, chances are you give up. Persistence is measured by the shareable output. According to my experience, quantity is more important than the quality for measuring persistence. It means if you are producing lots of so-so blog article or technical note and low quality draft of academic papers, you have the persistence. Think about growing academic muscle by writing everyday.\n  ","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"3f80f58bdfbdbbcdacb6f39ae8597a48","permalink":"http://crystal.uta.edu/~park/post/hdilab-culture/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/hdilab-culture/","section":"post","summary":"In HDILab, we learn by teaching. Do not study or work but create a sharable output.  Be true to yourself.  ","tags":["student advice"],"title":"The HDILab culture","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Contents  Introduce the role of the rewards in reinforcement learning  Use the example of dog training Explain the dopamine Explain the  Introduce an extrinsic rewards  Classic rewards Used in reinforcement learning examples  Atari Alpha Go World model paper   Introduce an intrinsic rewards\n Explain the limitation of the extrinsic rewards  Montezuma\u0026rsquo;s revenge  Explain the mechanism of the intrinsic rewards  How the curious or explorative behavior relates to the intrinsic rewards How to formulate the intrinsic rewards mathematically?  Model-free vs model-based reinforcement learning Intrinsic rewards examples  Introduce paper by Stanford World Discovery Model paper   Introduce the limitation of the intrinsic rewards\n The search space is too vast for a single agent learn by exploration\n Example: Radnom permutation of alphabet to generate a meaningful sequenc. Example: Musical note\n  Explain the higher order Markov chains\n   Introduce the social rewards\n Introduce the imprint Introduce how the baby learns  Learn by imitation Joint attention Gated learning  Theory based mathematically driven algorithm vs quick dirty rule based system  Future research direction\n Models that take into account the innate mechanisms Environments that can test the models with social rewards   Reward plays a key role in the reinforcement learning.\nIn this post, I will explain three types of the rewards that can shape the intelligent behavior.\n Extrinsic rewards: A reward triggered by the external entity. Intrinsic rewards: A reward that can be generated by the agent himself. Social rewards: A rewards that can be generated by the social activities  Extrinsic rewards Extrinsic rewards means the positive or negative reward triggered by the external entity. A concrete example is the Previously rewards meant mostly extrinsic rewards.\nIntrinsic rewards Social rewards A search space to generate a meaningful space is very vast and only very few are meaningful. But we can learn from others the meaningful sequence which reduces the search space\nThis can be expressed with music note.\nShould we use reward mechanism or reflex mechanism?\nImprint is an example of reflex mechanism. Because we have the freedom to follow or not, I think reward mechanism makes more sense.\nReferences  Jones, Susan S. (2012-12-10). \u0026ldquo;Human Toddlers\u0026rsquo; Attempts to Match Two Simple Behaviors Provide No Evidence for an Inherited, Dedicated Imitation Mechanism\u0026rdquo;. PLOS ONE. 7 (12): e51326. Bibcode:2012PLoSO\u0026hellip;751326J. doi:10.1371/journal.pone.0051326. ISSN 1932-6203. PMC 3519587. PMID 23251500 Jones, Susan S. (2009-08-27). \u0026ldquo;The development of imitation in infancy\u0026rdquo;. Philosophical Transactions of the Royal Society B: Biological Sciences. 364 (1528): 2325–2335. doi:10.1098/rstb.2009.0045. ISSN 0962-8436. PMC 2865075. PMID 19620104. Rowland, D.C., Yanovich, Y. and Kentros, C.G. (2011). A stable hippocampal representation of a space requires its direct experience. Proceedings of the National Academy of Sciences. 108(35). 14654-14658. -\u0026gt; An evidence for Gated language CONSPEC and CONLERN: a two-process theory of infant face recognition. J Morton, MH Johnson - Psychological review, 1991 - psycnet.apa.org Evidence from newborns leads to the conclusion that infants are born with some information about the structure of faces. This structural information, termed CONSPEC, guides the preference for facelike patterns found in newborn infants. CONSPEC is contrasted with a\n Newborns\u0026rsquo; preferential tracking of face-like stimuli and its subsequent decline MH Johnson, S Dziurawiec, H Ellis, J Morton - Cognition, 1991 - Elsevier Abstract Goren, Sarty, and Wu (1975) claimed that newborn infants will follow a slowly moving schematic face stimulus with their head and eyes further than they will folow scrambled faces or blank stimuli. Despite the far-reaching theoretical importance of this … How the brain processes social information: searching for the social brain TR Insel, RD Fernald - Annu. Rev. Neurosci., 2004 - annualreviews.org ▪ Abstract Because information about gender, kin, and social status are essential for reproduction and survival, it seems likely that specialized neural mechanisms have evolved to process social information. This review describes recent studies of four aspects of social Eye contact detection in humans from birth T Farroni, G Csibra, F Simion… - Proceedings of the …, 2002 - National Acad Sciences Making eye contact is the most powerful mode of establishing a communicative link between humans. During their first year of life, infants learn rapidly that the looking behaviors of others conveys significant information. Two experiments were carried out to demonstrate …  ","date":1564981200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"31334378b8fe8152420ce9ac4e6886cf","permalink":"http://crystal.uta.edu/~park/post/rewards/","publishdate":"2019-08-05T00:00:00-05:00","relpermalink":"/post/rewards/","section":"post","summary":"Extrinsic rewards is for the animal or bug level of intelligence. Intrinsic rewards or curiosity enables efficient navigation in the vast exploration space. Social rewards enable imitation or gated learning which reduces the exploration space.  ","tags":["agi","research"],"title":"Rewards for AGI","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Life evolves. The speed of evolution depends on the mechanism of the evolution. The first evolution was based on the random mutation of genes. It is basically trial and error. But the diversity of the life forms it generated is awesome. I would call it hard evolution. We, humans, are one of the successful life forms with mosquitoes and cockroaches.\nHuman adopted a new strategy. It was building a more flexible general purpose brain to adapt to the environment. Other life forms also have the brain. But its function was more rigid or hard encoded. You can think how the pocket calculator compares with the smartphone. Both have a similar electronic components namely CPU, memory and I/O devices.\nUsing the general brain, we can do many things that was not possible. But the essential function is learning. In hard evolution, the special variation that worked well leaves the genes. However, in soft evolution, we can learn from others. It basically allows exponential trial and errors. Nobody can experience all the diverse experience in the world given limited time and energy. Still we can learn from others first using verbal language. This allowed us to transfer knowledge from nearby people. The written language allowed it to go beyond the limitation of the space and time of the verbal language. Now what is beautiful about the soft evolution is that the time it takes to evolve has reduced dramatically. The soft evolution happens in the brain circuitry during the life time of the agent while in the hard evolution, the life time of the agent was a single unit of progress.\nBecause of this general flexible brain, we are born with a rather blank state compared to other animals such as cats and dogs. (Of course there are still many innate circuitry which I will talk later about.) And it takes relatively long time to have a reasonably working program. Interestingly as the amount of the information to function well in the society increases the time it takes to be independent is also increasing. Now it is common in the first world that it takes about 20 to 30 years before one can be independent from their parents.\nI said the brain is born as a basically blank state. But still there are innate mechanism to help learn from the parents. Think of the truly blank state agent. We got a continuous stream of image and sound. How we can make sense of the these images? For example, parents talking to the baby might look like a skin colored chunk with a few holes that are constantly moving. One hole continuously changes shape and make a sound (hint: mouth). How can we learn this chunk of image is a mom and the sound is the language which is different from random noise from the street? Epistemology is an old branch of philosophy devoted to this problem.\nI think the innate mechanism is essential for the new born baby to make sense of the world. Let me give you an example. Andrew Meltzoff and Keith Moore found that a baby who was about 7 hours since the birth can still mimic the tongue movements. It is quite surprising when we think about all the skills to do this. First, the baby has to recognize the human face. Then he has to map the human face he never seen to his face that he never seen himself. Finally the baby has to control the tongue to imitate the expression.\n New born baby can imitate the random facial expression. This capabilities an innate mechanism to help learn from others. We should learn from relevant objects such as people and care givers and not from irrelevant objects such as cars or doors. Image from Science 1977   I believe this imitation capability is essential to learn from others. Given blank state, we should not fill this blank with random noise but relevant contents. What will be the relevant contents for the new born baby. Most likely it will be from the parents and caregivers. The imitation mechanism has already filtered irrelevant objects in the world and focused on the human facial expression and followed its motion. This limits the vast space of the exploration to meaningful trajectory that was suggested by human.\nLessons for the AGI In the famous DQN paper, there are experiments for the Atari games. Among many games that showed superhuman performance by the artificial agent, there was one game that showed very poor performance called Montezuma\u0026rsquo;s revenge.\n  This blog explains why the Montezuma\u0026rsquo;s revenge game is particularly challenging. In a summary, there are too many things that have to be done sequentially such as getting the key, avoid the monster, and go back to the door. The random behavior by the agent cannot explore the vast exploration space in a limited time. In 100 Million iterations, the agent can access only the second stage. This shows the limitation of the random trial and error. I said other animals cannot learn. But I was wrong. They can learn. For example, dogs can learn new trick. They learn by trial and error. They associate behaviors with positive rewards or treats. But things you can learn from this approach is limited.\nI think there are three components for the artificial general intelligence. The first is a model. For example, in my HPM theory, the agent predicts the next sensory input with the help from the hierarchy to do the higher-order prediction (chunking). The second is an environment. It provides a source of information. The problem is there are vast space for exploration. Finally there will be many smart tricks such as imitation or social mechanism to focus on the human interaction during language acquisition to reduce this search space effectively by social learning. Our lab tries to build an environment that can provide a testbed for such social mechanism and study the models.\nFinally, what will be the next step for the evolution? I think it will be how we can limit the limitation of the brain cell. The brain cell is relatively small and degradable. If we can use our electronic components as a device, it will achieve a step-up in the evolution speed. But more importantly, I think the true power of the next evolution is when the artificial intelligence agent upgrades its own structure. This upgrades will be beyond the human\u0026rsquo;s ability to follow which is called technical singularity. Some would call it computational evolution.\n The evolution of the intelligence   ","date":1564981200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565117378,"objectID":"9e80d9eba7c40220c7be82fbc83fd54e","permalink":"http://crystal.uta.edu/~park/post/soft-evolution/","publishdate":"2019-08-05T00:00:00-05:00","relpermalink":"/post/soft-evolution/","section":"post","summary":"The first evolution mechanism was based on the DNA. The second evolution mechanism was based on the brain. The third  evolution mechanism will be based on algorithms","tags":["agi","research"],"title":"Soft Evolution","type":"post"},{"authors":["Deokgun Park","Steven M. Drucker","Roland Fernandez","Niklas Elmqvist"],"categories":null,"content":"  Sequence of layout operations to generate a unit column chart for survivors of the Titanic by passenger class.   ","date":1538016791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"f2aba7552741a49797bb1b7afbffe7d3","permalink":"http://crystal.uta.edu/~park/publication/atom/","publishdate":"2018-09-26T21:53:11-05:00","relpermalink":"/publication/atom/","section":"publication","summary":"Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark - a visual unit - during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.","tags":["Information Visualization"],"title":"Atom: A Grammar for Unit Visualizations","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"http://crystal.uta.edu/~park/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Deokgun Park","Seungyeon Kim","Jurim Lee","Jaegul Choo","Nicholas Diakopoulos","Niklas Elmqvist"],"categories":null,"content":"  The CommentIQ UI showing toggleable visualizations such as scatterplot, map, and timeline (left) that enable overview and filtering of comments, as well as an adjustable ranking based on various weighted quality criteria (right).ConceptVector supports interactive construction of lexicon-based concepts. Here the user creates a new unipolar concept (1) by adding initial keywords related to tidal flooding (2). The system recommends related words along with their semantic groupings (3), also shown in a scatterplot (4), revealing word- and cluster-level relationships. Irrelevant words can be specified to improve recommendation quality (5). Concepts (9) can then be used to rank document corpora (10). Document scores can be visualized in a scatterplot based on concepts such as tidal flooding and money (7). Users can further refine concepts based on results (8).   ","date":1517195878,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"b66bafc1a63aa7d16637aa7c7c4931de","permalink":"http://crystal.uta.edu/~park/publication/conceptvector/","publishdate":"2018-01-28T22:17:58-05:00","relpermalink":"/publication/conceptvector/","section":"publication","summary":"Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.","tags":["Text Analysis","Visual Analytics","Open-ended Tasks"],"title":"ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding","type":"publication"},{"authors":["Deokgun Park"],"categories":null,"content":"","date":1483250400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548263775,"objectID":"b114db2e37f3222e6421fb54941f18d1","permalink":"http://crystal.uta.edu/~park/talk/naver/","publishdate":"2017-01-01T00:00:00-06:00","relpermalink":"/talk/naver/","section":"talk","summary":"A comment section, such as for an online news article, video, or social media post, is commonly a place to share personal opinions. However, when there are too many comments, it is challenging to gain new insights and estimate the distribution of the public opinions simply by reading them. Selecting and promoting high-quality comments can mitigate those problems, but the required resources for choosing them manually are too high for common practice. In this talk, I will introduce my research on visual analytics for comment analysis. The ability to see an overview of the comments and to create custom rankings supports moderators, editors, and commenters themselves. I also propose a method to aid semantic analysis where users can build custom dictionaries for unique concepts and use them to analyze comments. In the future, assessing public opinions will play an important role in protecting the digital democracy against organized attempts to manipulate public opinions.","tags":[],"title":"Visual Analytics for Comment Analysis (in Korean)","type":"talk"},{"authors":["Deokgun Park","Simranjit Sachar","Nicholas Diakopoulos","Niklas Elmqvist"],"categories":null,"content":"  The CommentIQ UI showing toggleable visualizations such as scatterplot, map, and timeline (left) that enable overview and filtering of comments, as well as an adjustable ranking based on various weighted quality criteria (right).   ","date":1474950413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"f7a8d1ac5322c7c1c36cc05d2b9bf386","permalink":"http://crystal.uta.edu/~park/publication/commentiq/","publishdate":"2016-09-26T23:26:53-05:00","relpermalink":"/publication/commentiq/","section":"publication","summary":"Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.","tags":["Text Analysis","Visual Analytics","Open-ended Tasks"],"title":"Supporting comment moderators in identifying high quality online news comments","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"http://crystal.uta.edu/~park/project/deep-learning/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":["Deep Learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"http://crystal.uta.edu/~park/project/example-external-project/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Deokgun Park"],"categories":null,"content":" Hello,\nMy name is Deokgun Park and I lead Human Data Interaction Lab (HDILab) in the Department of Computer Science \u0026amp; Engineering at the University of Texas at Arlington. In this article, I will explain the problems we are solving at HDILab. In Human Data Interaction Lab, we are studying artificial general intelligence (AGI).\nMany students already have an idea about artificial intelligence (AI). But artificial general intelligence or AGI might be a new term. So let\u0026rsquo;s start with what AGI is.\nClarifying AI and AGI Since Marvin Minsky and other researchers gathered at Dartmouth College in Hanover in 1956, the goal of AI was to build a machine that can do many tasks like humans. But after experiencing a few AI winters, the academic focus shifted to building models to perform a single application because it was more tractable. This approach has been frequently called machine learning (ML). It is said that one of the reason ML has been coined was to avoid mentioning AI in the proposal during the AI winters. It was successful, and we achieved many advances in specific applications such as image classification, machine translation, self-driving car, or playing Go game. And the term AI became hot again.\nBut we still lack an idea about how we can build a general-purpose AI which appears in Hollywood movies or general public associates with. Because most of the current AI research is application-specific, we need another term for the original general purpose agent. It has been rephrased many times, including\n strong AI true AI human-like AI movie-like AI lifelong learning continual learning meta learning or learning to learn artificial general intelligence (AGI)  We will use AGI to refer this original AI because it tells the fundamental difference between the current mainstream AI research, which is application-specific.\n AI vs AGI. AI started as a general purpose model, but later changed its meaning to mostly application specific models. We will use artificial general intelligence (AGI) to mean the original general purpose model.   How to test AGI Before we discuss how we can build AGI, let\u0026rsquo;s start with how we can test if we built AGI. Because according to Peter Drucker or Lord Kelvin,\n If you can\u0026rsquo;t measure it, you can\u0026rsquo;t improve it.\n Alan Turing suggested a test to verify AGI. According to the Turing test, also known as Imitation Game, a human participant asks an agent hidden behind the wall to perform many tasks. If the human cannot discern whether the agent is truly a human or an artificial agent, then the artificial agent is assumed to achieve a human-level intelligence.\n In the movie Ex Machina, you can see how Turing test is conducted.   While theoretically valid, it poses several problems in its practical application. First, it is too difficult for the current status of research. The human tester can use all the knowledge about the world to test the agent, yet it would be very costly and impossible to teach all the knowledge about the world to the artificial agent, while researchers struggle to discover how we can mimic human-like learning. It is like asking a 1st grader to take SAT test.\nSecond, the test does not provide any idea about where the model can learn those knowledges required to take test. It is like testing students without giving them any textbooks or lectures.\nThird, even for the same model and the same evaluator, the feedback will be different from today to tomorrow. The test is subjective and not reproducible.\nFinally, it is prohibitively expensive to hire people to conduct a test. Humans may participate in annual Turing test competition, but during the hyperparameter tuning or iterative model refinement, it is difficult to get access to human testers. For these reasons, it is not practically applicable in the current renaissance of the AI.\nWe propose an alternative test method for AGI. This method is based on the observation that even if we raise cats and dogs, treating them like human babies, they cannot learn to speak. The animals are capability-limited, and the human baby cannot learn to speak if it is separated from people speaking to it. It is environment-limited. Therefore we can think that the language acquisition is the function of the environment and the capability of the learning agent. In other words, if an agent can learn how to speak in a proper environment, we can say that it has the capability to do artificial general intelligence. We call it the Language Acquisition Test for AGI or Park\u0026rsquo;s test.\n In Park\u0026rsquo;s test, the agent is said to have a human-level intelligence if it can learn language given the proper environment   To pass a Park\u0026rsquo;s test, we needs a capable model and a proper environment. Let\u0026rsquo;s look at environment first.\nThe Environment for the Language Acquisition To conduct the Park\u0026rsquo;s AGI Test, an agent requires an interaction with the world to learn how to speak. One factor that enabled the recent advances in reinforcement learning was the use of the simulated environments such as Atari games or 3D first person shooting games such as VizDoom. Those environments can be used to build and test an agent that can optimize its behavior with little instructions while maximizing the reward signal as a goal to train an agent. However, environments and reward signals adopted in those studies are primitive and more suitable for the development of low-level intelligence which can be found in fish or bugs.\n               Atari Environment Car Racing Environment RoboSchool Environment    If we want to build an agent with human-level of intelligence, we need to use an environment that can provide a reasonable sensory input and feedback that can teach an agent how to speak. One naive way to provide such an environment is using real people to provide the required responses or trainings. However, using real people has similar limitations with the Turing test. We will have to explore many models in a trial and error before we can finally build an AGI. In addition to the prohibitive cost of using human participants, human experimenters will become quickly tired of providing the same feedback again and again or providing different feedback to the same situation which does not lead to reproducible research.\n   Advanced Discussion: What is language?     Some would claim that the poor baby abandoned in jungle will still develop a language to communicate with other animal. Similarly there is an emergent communication pattern among collaborative robots. While true, we are interested in human language in this project especially for human robot interaction. The rationale is that we want the robot to learn human language not vice versa.    There are a few prior researches in language acquisition. Devendra Singh Chaplot and other researchers in the Carnegie Melon University used VizDoom environment to demonstrate how agent can learn semantic concepts using reinforcement learning. In their experiments, agents get rewards when they go to objects according to the verbal direction such as \u0026ldquo;Go to short blue torch\u0026rdquo;. What is interesting is that during the training the agents experience verbal direction such as \u0026ldquo;Go to blue torch\u0026rdquo;, \u0026ldquo;Go to torch\u0026rdquo;, \u0026ldquo;Got to short red torch\u0026rdquo; and so on but never experience \u0026ldquo;short blue torch\u0026rdquo;. Even then during the test time, they can follow the direction to go to \u0026ldquo;short blue torch\u0026rdquo;. This implies that the agent can map the words such as \u0026ldquo;short\u0026rdquo;, \u0026ldquo;blue\u0026rdquo;, \u0026ldquo;torch\u0026rdquo; with the visual features in the objects. This is a step forward to solve the symbol grounding problem.\n Agents learns language using reinforcement learning. Image by Devendra Singh Chaplot et al   But the concepts that can be learned in this way is limited. There are limitations due to the environments. We don\u0026rsquo;t learn by fetching objects according to the parent directions. It will take too long to learn all concepts this way. We need a better environments that can teach the language\nWhen the transistor was first invented, people were excited to use it as an amplifier to build radio and radars. But the true power of the transistor was when it was arranged in a specific structure, it could build a logic gate. By arranging logic gates in a specific way, we could build a CPU, the true ultimate power of transistor.\nThe same goes with the connectionism. We know how one or two neural cell behaves exactly and we can simulate them. But most advancement came with the arrangement of this device in a specific ways, such as multi-layer perceptron (MLP), convolutional neural net (CNN), recurrent neural net (RNN), and generative adversarial network (GAN). In this sense, we are experimental computer scientist who seek the right structure mostly by trial and errors. Because of this nature, it is helpful to have an environment that can provide an easy, low-cost, and reproducible way to experiment.\nIn our lab, we would like to develop an environment for Park\u0026rsquo;s test. The key idea is that we will focus on the critical stage of the human development when humans learn how to speak as infants. This environment will contain a 3D replication of a room in the home with a few toys. There will be a mother character and a baby character. The mother character will be programmed manually using traditional game AI technology to take care of the baby and lead a conversation with the baby character which is often called a baby talk, child-directed speech (CDS), motherese or infant directed speech (IDS). IDS is a communication pattern, when a mother tries to teach language to an infant. The baby character will be the learning agent. The success of the test will be decided by whether the learning agents can acquire language and develop reasonable behavior comparable to the human developmental progress.\n Previous environments have led the advancements of the reinforcement learning. We propose a novel environment for the language learning.   Hierarchical Prediction Memory (HPM) HDILab also studies the model that can learn the language in the environment described above. Let\u0026rsquo;s start our discussion from the function and the mechanism.\nFunction and Mechanism The most difficult way to fly is to imitate the biological mechanism of flying such as in birds or flies. The biological mechanism is highly evolved due to the long history of adaptation. And many of the complex mechanism is due to the biological limitation which does not apply to us. What we need to figure out is what those mechanism are actually doing or function. The way human or horses run is pushing the ground backward. The way birds fly is pushing the air downward. This applies to the intelligence. The biological mechanism is very complex. It has been very optimized. It has to use biological lossy devices. So the mechanism is very complex. But we need to learn what is the function of the mechanism. It is in my opinion prediction of vector sequence. Hierarchy helps to overcome the temporal limit by chunking. The essence of intelligence is hierarchical prediction of vector sequence.\nWe conjecture that the essence of the intelligence is a hierarchical prediction.\nWhere the original connectivist has missed? The original connectivist thought the decision or pattern recognition as the core activity. However, this lead to the wrong formalization of the problem? In my opinion, prediction should be the core activity. Prediction includes the decision or pattern recognition, but the main difference is that it has the streaming input and streaming output.\n","date":1461128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"59de895a25b9e24e0fe72f00b1f2227a","permalink":"http://crystal.uta.edu/~park/post/research-in-hdilab/","publishdate":"2016-04-20T00:00:00-05:00","relpermalink":"/post/research-in-hdilab/","section":"post","summary":"Artificial General Intelligence (AGI) Research at Human Data Interaction Lab (HDILab)","tags":["agi","research","student advice"],"title":"AGI Research in HDILab","type":"post"},{"authors":["Deokgun Park","Jungu Choi","Niklas Elmqvist"],"categories":null,"content":"  The MovieVis tool. Two groups in the movie space have been selected to compare corresponding user distribution. Two movies selected in the upper-center region—One flew Over the Cuckoos Nest (1975) and Amadeus (1984)–and are shown in blue color. Another two movies selected in a lower-center region—Phenomenon (1996) and Twister (1996)—are shown in orange. The highlighted users are those who liked all both pairs of movies (because the group mode is set to common). Based on the user space axes—gender for the horizontal and age for the vertical—we can see that while the movie One Flew Over the Cuckoo’s Nest and Amadeus were favored by male reviewers of all ages, the Phenomenon and Twister were liked by relatively younger male audiences.    On the left, we compare two movies, Toy Story (1995), in blue, and Scream (1996), in orange, according to the age, location and similarity criteria for users. Some notable observations are while the former is liked all around the U.S. by any age groups the latter is mostly popular in the eastern part and within a younger generation. On the right, we compare two users, a 19-year-old male student, in blue, and a 51-year-old male educator, in orange according to the average, release date, and similarity criteria for movies. We observe that the older user tends to rate older films highly. In addition, his average review tends to conform to the average ratings patterns of all users while the younger user seems to deviate from it.   ","date":1459266011,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"6dfd7923368efd00696a61be7c219db1","permalink":"http://crystal.uta.edu/~park/publication/parallelspace/","publishdate":"2016-03-29T10:40:11-05:00","relpermalink":"/publication/parallelspace/","section":"publication","summary":"We present ParallelSpaces, a novel method to explore bipartite datasets in both feature and data dimensions. This dyadic data is displayed as weighted bipartite graphs using scatterplots in two separated visual spaces, where each entity is positioned according to multi-dimensional properties of each entity or similarity in preferences. Selecting or navigating in one space is reflected in the other space, so that organic visual patterns can be formed to facilitate the characterization of underlying groupings. To aid visual pattern recognition we also overlay a contour plot based on kernel density estimation. We have implemented two instantiations of ParallelSpaces for (a) movie preferences, and (b) business reviews as Web-based visualizations. To validate the method, we performed a qualitative user study involving eleven participants using these Web-based tools to explore data and collect deep insights. ","tags":["Information Visualization","Visual Analytics"],"title":"Parallelspaces: Simultaneous Exploration of Feature and Data for Hypothesis Generation","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"http://crystal.uta.edu/~park/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]