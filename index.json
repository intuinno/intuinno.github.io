[{"authors":["Deokgun Park"],"categories":null,"content":" Human level intelligence test I propose a language acquisition as the test for the human-level intelligence. If we raise baby animals like a human baby, they will not learn human language. They are capability limited. If a human baby is raised in the jungle without any human interaction, they cannot acquire human language. They are environment limited. We can say that the language acquisition is the function of the capability and the environment. Therefore, if any agent can learn language given the proper environment, we can say that the agent has the capability for human level intelligence.\nLanguage acquisition environment For the proper environment, we need other humans to teach language. There are many social mechanisms that enable this. Caregivers use motherese or infant-directed speech (IDS). The baby attends where the caregiver attends (joint attention), and the baby imitates what they see. To provide artificial agent this environment, there are two approaches. The first is using physical robots and asking the real humans to be caregivers. Given the current state of the art, this can be cost-inhibitive and not reproducible. The second is using simulated environment, but programming the caregivers to teach diverse and reasonable responses to the random behaviors of the learning baby agent is a scientific challenge. I plan to overcome this by limiting the scenarios and contexts for the environment and approximating mother behavior. As a concrete example, let’s say that we invite 100 pairs of the mother and child to play on the play at with a few toys. We capture their motions using motion capture system and build the behavior library. For example, if 20 babies touched duck toy during the experiment, we can record 20 responses of the mothers. In the simulator, when learning agent touches the duck toy, the mother character will play one random behavior out of 20 possible reactions.\nModel development Using this environment, we will develop a model that can learn by observing and interacting with other human-like pre-programmed agent. There are two parts for the model.\nFirst, we need a common cortical algorithm that is a universal machine given a vector as an input that can predict next state vector. Hierarchy plays an important role here to overcome the trade-off between long-term predicton and training data. Some candidates for the possible algorithms are below - Clonned HMM by Vicarious - HTM described in the “Why Neurons have Thousands of Synapses, A Theory of Sequence Memory in Neocortex” by Numenta - Hierarchical Prediction Network (HPNet) in “A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal Sequence Learning and Prediction” by Qiu et al.\nNote the vector representation changes from Sparse binary (HTM) to dense continuous (HPNet). Personally I prefer sparse binary but it can be functionally equivalent as in the electric motor and combustion engine in the vehicle. We also need to batch multiple modules in a heterarchical way as the society of minds. A good example is the contour-surface factorization in the RCN paper. We will need many more modules to handle sensory motor, auditory and so on. The executive functions on the prefrontal cortex can be built on the same principle while working on higher layer vectors that was generated as final output vectors in other modules.\nSecond we need a supporting innate mechanisms to help these universal modules learn from the world. I believe the reward signal plays an important role here. We can learn lots of lower layers from the principle of predictive coding or intrinsic rewards where the action from the agent generates the next sensory inputs by training modules to predict next states. However, there are too many exploratory space in the world that an agent cannot learn from trial and error. The key in observational learning is studying innate mechanisms that can guide the learning agent to observe other humans and learn from them. For example, there are social rewards which can signal the learning agent that this is important sequence and needs to be memorized with increased focus. Newly inborn at least attends other humans speaking motherese more than other static objects or humans. There should be some kind of BIOS that can fill the contents of the blank state in the universal modules with meaningful content.\nRoad map Once we have a model that can do observational learning in the simulated environment, we can port this model to the embodied robots and use real humans to teach the language. I believe that the model that can learn the vocabulary of two years can be easily extended to the reading level because it learned the concepts grounded in the sensori-motor sequence.\n","date":1575612000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575657129,"objectID":"11361d252193bbcfe578d8eb103329a9","permalink":"http://crystal.uta.edu/~park/post/observational-learning/","publishdate":"2019-12-06T00:00:00-06:00","relpermalink":"/post/observational-learning/","section":"post","summary":"Below I summarize my plan for building artificial general intelligence (AGI). I start with the test for AGI, and then goes into environments and models for it.  ","tags":["agi","research"],"title":"Observational Learning: How I would build AGI","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Olfactory learning Nowadays you hear artificial intelligence or machine learning frequently.\nWhat is the difference between intelligence and learning?\nGeneral words such as intelligence and learning are difficult to define. Rather than trying to define, comparing the meanings can be helpful to understand both concepts.\nIntelligence is the rules to active actuators according to the sensory input to improve one\u0026rsquo;s wellbeing. Learning is acquiring new rules after birth. In this sense, intelligence does not need learning.\n There can be intelligence without learning and intelligence with learning.   Bees that show intelligent behavior does not rely on learning. All their smart rules are encoded in the genes. Similarly my calculator is intelligent. But I don\u0026rsquo;t think they are learning.\n This calculator is advertised as intelligent calculator, but it does not have the ability to learn new trick.   While trying to build an AI, humans tried first to infuse rules by hand. But soon they found there are two fundamental problems. First, there are too many rules. Not only the number, many rules was contradictory each other. Second problem was the symbol-grounding problem.\nAfter some failure, we arrived at the consensus that to build human-level intelligence, machines needs to learn itself. Interestingly, this idea of learning machine appears in the Alan Turing\u0026rsquo;s seminal paper, Computing machinery and intelligence (1950).\n Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child\u0026rsquo;s? If this were then subjected to an appropriate course of education one would obtain the adult brain. Presumably the child brain is something like a notebook as one buys it from the stationer\u0026rsquo;s. Rather little mechanism, and lots of blank sheets. (Mechanism and writing are from our point of view almost synonymous.) Our hope is that there is so little mechanism in the child brain that something like it can be easily programmed. The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child.\n One way to think about the AI and AGI is where the general part comes. General part comes from the ability to learn new skills. Therefore learning is a required condition of the AGI, too.\n   What is the sufficient condition for the AGI?     I think a language acquisition or learning is a sufficent condition. Can an agent learn language by trial and error? You can read more about this    Learning is very expensive.\nMost animals cannot afford it.\nIf you make a mistake, you die. And there is no way to transfer your knowledge to descendants using genes. Your exploration to the world cannot benefit the survival of your genes. How dare animals begin learning?\nThink of cars. Modern luxury cars have lots of complex modules such as adaptive cruise, ABS, regenerative braking, hybrid motor, all wheel drive, GPS navigation, 8 way sound system, memory seat, AC and heater and so on. If anyone try to build a car one hundred years ago, will he consider all the complex modules of the modern cars? Probably not. He will only focus on how to rotate wheel without human manual muscle. That\u0026rsquo;s why the first cars had barely Minimum parts to implement a function. They had a motor and battery connected to wheels. Or they had an steam engine.\n The first car accident   So if we look at modern human brain, there are lots of fancy complex modules interconnected.\n Components required for reaching and grasping. There are many modules for controling hand gesture. Image from Schema theory by Michael A. Arbib (1998)   Then if we want to reverse-engineer learning, trying to copy from the human brain is trying to learn how to build a first car from the modern luxury cars. Let\u0026rsquo;s properly assume you don\u0026rsquo;t know what CPU and memory are because you are from one hundred years ago. The modern cars have 50-100 CPUs. Can we make sense what\u0026rsquo;s going on here?\nAll of previous arguments are to claim that we need to look at the minimum parts to implement a function to build a learning machine.\nOlfactory system is a candidate for such a system. We belive it is the minimum parts because it is oldest. How do we know it is oldest? Because it goes back to the lizard.\nMany readers might got the idea when I mention that lizard. They can skip the next part. But for the rest who do not have a clue, let me introduce crash course on the brain evolution.\nWhen we look at the brain there are many parts just too many parts. But if we simplify mercilessly, there are three major parts, hindbrain, limbic system, and neocortex.\nHindbrain takes care of basic tasks that should be automatic. No learning should occur here. We should not learn how to breathe or maintain body temperature. It should be built in. Otherwise our chance of survival will be too thin. This came first in the evolution and it is deep inside. The last part, neocortex is where the human level intelligence is happening. It begins with the blank state and fills as we learn. This came last in the evolution and located in the surface or outside.\n The Triune Brain clusters brain into three major parts as hindbrain, limbic system, and neocortex.   Limbic system is what is in the middle. It came second and located in the middle between hindbrain and neocortex. This simplication is called the triune brain. Previously limbic system is claimed to be old mammalian brain and it is believed to handle emotions. However, it is discovered in the common ancestors of reptiles and mammals.\n The Limbic system is composed of olfactory sensor, hippocampus, and amygdala   It is composed of olfactory sensor, hippocampus, and amygdala. Olfactory sensor is simple chemical sensors. The neurons in the olfactory sensors fire according to the chemical input. Amygdala is taking control of emotions. You can simplify emotions as a change of body reactions to the same stimulus depending on the chemical concentration in the animal.\nAmygdala controls the release of the chemicals. In this sense, it is an actuator.\nTo make situation ruthlessly simple, let\u0026rsquo;s say there are only two reactions, to approach or to avoid. (Of course there are others such as mating or fighting.) Let me clarify the same stimulus part to avoid confusion. Of course when we see food, we approach and when we see a tiger, we avoid. But let\u0026rsquo;s see you see an unidentified object, something you never seen or know. If your blood has more approach chemical, you will approach it. While if your blood has more avoid, vice versa. This thing, unidentified object, is important concept in the learning. Your gene may hard code the food and the predator. You don\u0026rsquo;t need learning to handle that case. But it is when you have a new stimulus that requires learning. Which is a learned reponse and different response for the same stimulus.\nAs a concrete example, when a mouse hears a certain bell and gets the food, he associates approach with the bell sound. But if a poor mouse next cage gets the electrical shock to the same bell sound, he will associate the sound with avoid reaction.\nSo we have a sensor and an actuator. What else we need to build a learning system? Learning system by definition has to have a mechanism to store input and connect or associate appropriate actions. Hippocampus is the one who is taking care of the associative memory.\nAs a summary, the minimum parts to implement learning is a sensor, an actuator, and a memory. Let\u0026rsquo;s call it a minimalistic learning module.\nDo you have an experience that when you smell something, it reminds me of your home or childhood memory? It is because the olfacory sensors and hippocampus are very directly connected compared to complex routing in other modules in the brain. We guess that it was the first sensor that connected to learning module because the olfactory sensors are directly connected to the hippocampus while other systems are connected through thalamus. Thalamus is the information hub. There are many sensors coming in and coming out.\nOne interesting question to ask is why olfactory sensor was connected to this first learning machine.\nThe need to learn new food source might be an motivation. But why animals rely on the odor rather than the visual shape or sound? My guess is that chemical ordor has far lower dimension than other sensory input such as vision and auditory signals. To generalize over the visual system, we need far more processing and storage power. For example, a visual signal can be composed of 640 by 480 pixel value while odor signal can be characterized by 10 numerical values.\n","date":1568869200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568909889,"objectID":"3514a9170cdb12691afb327df23446d8","permalink":"http://crystal.uta.edu/~park/post/olfactory-learning/","publishdate":"2019-09-19T00:00:00-05:00","relpermalink":"/post/olfactory-learning/","section":"post","summary":"Intelligence is the rules to actuate actuators according to sensory input to improve one's well-being. Learning is acquiring new rules after birth. Learning requires sensors, actuators, and memory. Olfactory learning is the first learning that can give insight to the minimum parts for learning. ","tags":["agi","research"],"title":"Olfactory learning","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":"Below is adopted from forward in my Ph.D disseration.\n A man\u0026rsquo;s character is his fate. - Heraclitus\n It all began with the naıve and lazy man’s dream. I wanted interesting information to come to me even though I didn’t know if such information existed, and I didn’t search for it. I am lazy but addicted to information. I am also a Maximizer. According to the book The Paradox of Choice: Why More Is Less by Barry Schwartz, a Maximizer is the kind of person who scans all the available cereals in the supermarket and tries to select the best one. Frankly speaking, I envy the Satisficers because they will choose whatever option meets the requirements and forget about the rest.\nThat’s how I initially became interested in recommender systems, which are a class of algorithms that recommend something to my taste, as Amazon and other web-based services that are trying hard to extract more money from us. Also, from my previous experiences with wearable technology, I knew that the ability to extract valuable information from data will be the key component in the so-called big data value chain. Without the ability to turn data into insights, big data is just investment and cost. Analytics will be what generates revenue and profit.\nHowever, the journey never goes as expected and you never know where you will end up when you are setting out. Such was my Ph.D. I started with recommender systems, but the recommendations they make are not satisfactory due to limitations in the quality of these algorithms. It may be rather contentious to suggest that recommender systems algorithms are limited. Indeed, the world is changing and you never know if the next big scientific breakthrough will improve them to be more effective. But as anecdotal evidence to support my claim, Netflix never ended up using the state-of-art algorithms from the famous 1 million dollar competition. The algorithm showed top-performance, but the performance gain was not meaningful to justify the algorithm implementation cost.\nThat’s when I started to look for alternatives. Did I already tell you that I am a Maximizer? Therefore I concluded that we need to amplify the cognitive ability of the user to tackle the challenges of big data value creation. That was the focus of my Ph.D., presented here, with five design studies. The ideal ending will be that I am satisfied with my methodology and live happily ever after. But after six years of study, I see some fundamental limitations in the visual analytics (VA) approach as well. Those limitations are 1) VA systems are application/domain specific, 2) dependence on back-end algorithms that usually rely on the bag-of-words model, and 3) the requirement of user intervention (this can be both desirable and undesirable. But I am rather lazy.)\nSo here I am packing light again to find more fundamental ways to achieve my dream. Recent advances in neural networking look promising, and, being inspired by those advances, I want to build upon them to start a new journey into this untapped area. I am excited, afraid, humble, and foolish on this new journey.\nIf you want to know more detail, you can read my research statement.\n","date":1568782800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568828055,"objectID":"2440c50f07d8728a6a3146a13ec5b142","permalink":"http://crystal.uta.edu/~park/post/intellectual-journey/","publishdate":"2019-09-18T00:00:00-05:00","relpermalink":"/post/intellectual-journey/","section":"post","summary":"I was lazy and news-addict that I wanted news story that suits my interest flows towards me. I started with recommender system but soon found that it cannot deal with the delicacy of human language. I studied visual analytics to combine human intelligence with machine learning for the text analysis. But still found it unsatisfactory. Now I am studying grounded language which lead to artificial general intelligence research.   ","tags":["agi","Visual Analytics","research"],"title":"My intellectual journey","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":"My job is thinking and talking.\nI do reading and writting.\nI like learning and teaching.\n","date":1568264400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568308104,"objectID":"6886a023107d193afaeaa99b1831a4c4","permalink":"http://crystal.uta.edu/~park/post/a-scholar/","publishdate":"2019-09-12T00:00:00-05:00","relpermalink":"/post/a-scholar/","section":"post","summary":"A poem about my job","tags":["poem"],"title":"A scholar","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":"Listening to an old professor\nwho devoted his whole life\nlooking for the holy grail\ncouldn\u0026rsquo;t find it\nhanding over scattered clues\nto young, ignorant yet ambitious students\nlike once he was\nin the hope that allowed more time\nhe could finally see how minds work\nknowing that his time is over.\n","date":1568264400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568828055,"objectID":"4f2063cd924b89a9edb7c098bdb471d0","permalink":"http://crystal.uta.edu/~park/post/society-of-mind/","publishdate":"2019-09-12T00:00:00-05:00","relpermalink":"/post/society-of-mind/","section":"post","summary":"A poem about the society of mind (book) by Marvin Minsky","tags":["poem"],"title":"The Society of Mind","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":"I get many emails from master students who are interested in reserach.\n I would like to get research experience in my lab. May I join your lab as a master student?\n Below you can find my advice. TLDR, if you plan to pursue PhD after your master, it might be a good idea to experience research in my lab. If you plan to industry career, it might not.\nYou can contact me and discuss your interest. However, if you want to get a job after graduation, doing research with me might not be helpful. You might get a better chance for the job by preparing the coding interview. The research takes time to produce a meaningful outcome. And the research is a full-time job. For example, it is common that it takes more than two years before a phd student can produce a paper or meaningful software. However, this can be too late for the master students who need something to show to potential employer during first semester and third semester. Doing research is more beneficial for those who consider phd program after graduation.\nStill there are many masters student working in my lab. I encourage you to contact them.\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565105936,"objectID":"6e6fe2f39189ef68295dd2273703753e","permalink":"http://crystal.uta.edu/~park/post/master-research/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/master-research/","section":"post","summary":"If you plan to pursue PhD after your master, it might be a good idea to experience research in my lab. If you plan to industry career, it might not. ","tags":["student advice"],"title":"Advice for the master students who want to do reserach in my lab","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Recently, I got an email from a smart undergraduate student who was interested in research. I think his situation might be a common case so here I added my reply in the hope that this can be helpful to other students. Below is the modified email message.\n I have been talking with few graduate recruiters and found out that graduate schools don’t offer scholarship unless I am doing PhD or thesis (which I had no idea what that meant). All I wanted to do was to get a masters degree to gain enough knowledge on AI and deep learning and work for companies like DeepMind or Neuralink to bring AGI and ASI to real life. And now I am overwhelmed and lost with what I’m supposed to do. My options are to loan my way through graduate school (but I can’t do that since I am already on a huge debt for my undergraduate school), or get a PhD (this will take me forever and I will never be able to pay my loans back with the stipend), or find a job (until I am able to pay by loans and fund my graduate school but I really want to be on AI). I recently figured out that my major (computer engineering) wasn’t right for me. I decided to switch to computer science but I needed to take more classes so I am switching to software engineering to graduate on time with right the classes. My mind’s all over the place and I have no idea where to seek help from. I’m only halfway through the book even though I love it. I have been dying to learn Reinforcement Learning but I haven’t got time even to start searching about the resources.\nThank you so much, Dr. Park! I hope you have a wonderful rest of the day.\n Below you can find my advice. TLDR, it is okay to get a job if you are on a debt. If you think you are interested in the research, try to study on your own for a year or two. If you are still interested, you can consider a PhD.\nMy answer to the student Hi, Tomas (Name changed for privacy)\nI understand your position.\nBelow is my advice. The choice is yours. There is no right or wrong choice. The choice is based on who you are and in return shapes who you will be.\n to loan my way through graduate school (but I can’t do that since I am already on a huge debt for my undergraduate school),\n The graduate school here must be master’s degree, right. I don’t recommend it, either. You cannot get enough knowledge on AI topic and you don’t have enough time to create a knowledge that will make you attractable hire for companies like Deep Mind.\n or get a PhD (this will take me forever and I will never be able to pay my loans back with the stipend),\n It does not take you forever. It is actually somewhat short to do what I describe above. It will pay you back reasonably well if you are successful. But the decision cannot be made solely on financial reward because this financial reward is probabilistic. Better motivation would be the pursuit of knowledge or curiosity. And the reward is auxiliary. For example, you usually do not pursue professional baseball career because most of baseball related careers are not well-paid. Only winner takes it all.(I hope the chance is higher in the CS.) But many still pursue it because they enjoy baseball anyway.\n or find a job (until I am able to pay by loans and fund my graduate school but I really want to be on AI).\n This is an actually good option. Get a job. Get decent salary and pay your loans. The trick is that you can study AI on your own with online learning education resource nowadays. This can be sometimes better than what the Master’s degree can offer. When you study enough and you have a non-trivial question, it will be a good time to pursue PhD. Of course, consult with professors about your question.\nAttending academic conference is a good investment. Studying alone is actually harder than you might think. You will be busy for the job (actually everyone is busy, right? Even my 9th grade daughter is busy because she has to watch three hours of youtube videos in addition to all the school work and violin and archery practice) and you may find the AI study is not so fun. The good news is that if you don’t like it, you don’t have to do it. It means that you are probably not good fit for the PhD. Don’t worry. You have a job already and you can focus on the software engineering career which can be also very rewarding. According to my opinion, I think about 5% of the people with good school grades are actually good fit for the PhD. The main reason is that we get good grades by learning what others have found. But PhD is about finding new knowledge which is very different from learning what others have found.\nAs a summary, I think you have a potential. If you really want to study AI and deep learning now, you can get a PhD now. If you are not sure, it is okay to get a SWE job. If you find that you keep studying on AI and DL for one or two years on your own, you might consider getting a PhD.\nRegards\nDeokgun\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568308104,"objectID":"d836ea05e94048f86891c48151975cb2","permalink":"http://crystal.uta.edu/~park/post/graduate-school/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/graduate-school/","section":"post","summary":"If you are not sure, getting a SW job is okay. If you study AI by yourself for one or two years, you might consider applying for PhD program.","tags":["student advice","career"],"title":"Advice for the undergraduate students who consider the graduate school","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Recently, I got an email from a student from HDILab. I think his situation might be a common case so here I added my reply in the hope that this can be helpful to other students. Below is the modified email message from the student.\n Hello Dr.Park,\nI am trying to implement carracing. I am using a virtualenv python3.5.4. I am trying to run extract.bash using $sudo bash extract.bash. However, it returns\n” File “extract.py”, line 8, in  import gym ImportError: No module named gym” when extract.bash is trying to run extract.py.\nThe problem is I installed gym and box2d-py too, and yet it shows the import error.\n$ which pip /home/aishwarya/WorldModelsExperiments/carracing/venv/bin/pip\n$ which python\n/home/aishwarya/WorldModelsExperiments/carracing/venv/bin/python\nI’ve also tried the solutions given in here and in here\nCan you please help me out with this issue.\n Below you can find my advice. TLDR, if you have a technical problem. Solve it on your own while producing shareable outcome. If you have a research idea, your advisor can help you evaluate whether it is intractable, already done, or trivial.\nMy answer to the student Hi, Tomas (Name changed for privacy)\nI don\u0026rsquo;t know about this. You should check Google or ask other person\u0026rsquo;s in the lab. I think Jane and John might know.\nDon\u0026rsquo;t worry. Debugging this kind issue is common. I myself always deal with this issue. One of the reason, I cannot help is that the reason can be diverse. It is case by case. Therefore it is more important important to know how to deal with these kind of issues than the actual specific solution. There will be hardly anyone who can solve your technical problem with a clear answer. If you have one you are lucky. But still if you ask to much questions to him, you are actually costing a precious resource to the group which is a bad news for you. Below are some general advices.\n Use slack general channel. If you send email to one person, the chance is low that your busy and ignorant professor can help you. If you use slack to post your issue to the entire group, some competent students might help you.\n Make environment simple. 90 Percent of these issues are package version collision. Virtual env helps but not perfect. In old days I used to format frequently. That\u0026rsquo;s why I use one disk for system and one disk for home directory. Nowadays learning how to use docker will pay you back well in the long run.\n Try to learn generalizable lesson and share it. Most of time, you just fix and forget. You might try stack overflow or Google solution until you fix it. The problem with this approach is that the time you spent on solving this issue does not produce any credit for your work. While I also sometimes do that, the correct way to handle an issue is that first understand the reason of the issue and create a technical note or blogs that explains the issue and the solution. Many people in the lab can benefit your contribution and your technical blog will help building your professional reputation. You need to be accustomed to solve these kinds of issues. It is like growing your muscles.\n  How to use your advisor If your advisor cannot help you solve your technical issues, where can I use him? One area is when you have a research question. As a graduate student, you will always think the research question or the topic for the next paper. There can be two major pitfalls for setting the research topic. An academic work is about finding a knowledge that none has found before. There can be two reasons why nobody has done it before.\nFirst, it can be too difficult to do or trivial but not meaningful. When we think a new idea, I can bet there will be more than 10 people who have thought the same thing. Still the finding the solution might be too difficult. Time machine, teleportation, or thinking machines are those areas. Those are technologies that people wish but it is very hard to make a progress in those area. Or more commonly, your topic is slightly general and there can be many approaches to it. However, the topic might be well-known in the community and almost all low hanging fruits are taken already and what remains might be too challenging.\nSecond, your problem might be too narrow or trivial or already solved before. Your advisor is like a guide to the Mt. Everest or a chaperone to the party. He has an experience and background knowledge about the research community.\nTherefore getting a cross check of your idea can be a good use of your advisor.\n","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565102669,"objectID":"dbd9435e0f3601e2c69d958eac3fb883","permalink":"http://crystal.uta.edu/~park/post/how-to-use-your-advisor/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/how-to-use-your-advisor/","section":"post","summary":"If you have a technical problem. Solve it on your own while producing shareable outcome. If you have a research idea, your advisor can help you evaluate whether it is intractable, already done, or trivial. ","tags":["student advice"],"title":"How to use your advisor","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Below is HDILab culture. A culture is a principle about how we do everyday things. I hope that our culture shape our group in a productive way.\nWe learn by teaching. Every week we will have a paper reading seminar where one student will teach a paper to other lab members. A pizza and a diet coke will be provided. You will have a chance to announce what you are working on nowadays in a friendly environment 30 minutes before the seminar. Currently the seminar starts on Wednesday 12:30pm.\n After you have done lab seminar, add the slides and all materials into the Google drive here. The goal here is to make it easier for a new lab member to follow what we have done so far. Also write one paragraph abstract for the seminar to summarize what you teach. Add your call sign so that we can know who did the seminar. Students should answer the questions from colleagues.  Team play There are focused hours in the lab where students are supposed to be at the same space and time. The main idea is to have an informal chat, update, questions and answers. The focused hours are currently held Mon, Tue, Wed 10am-2pm at ERB 205. Don’t be late. If you think you will be late, report it to the slack.\nSharable output Students are required to maintain a lab journal and project journal Lab journal is for the general purpose work and project journal is for the project related progress. For example,\n I set up the new server or hardware for the-\u0026gt; hdilab journal I organized the inventory -\u0026gt; hdilab journal\n I replicated the world model -\u0026gt; daivid journal  Every work should be recorded and reproducible. Basically journal is for yourself. But write it well when others have to find something you did. When you accumulate enough work and progress, make them shareable and transferable chunk into three places:\n journal how-to technical note When you create a new technical note leave a author call sign such as #deokgun so that I can find who wrote what. If you need to update it, add a call sign and date after the original author.  Example Created: #deokgun 01/08/2019 Updated: #sanjay 01/09/2019   Personal homepage blog article Github readme.md  One common mistake for the new member is writing a technical note about a topic that is already covered in previous technical note from other members. This is problematic in the two sense. First, it means that you did not search or read other\u0026rsquo;s technical note. A value of the technical note is how many times it is read by others. If no one reads others\u0026rsquo; technical note, the value of our journal will be nothing. Second, you are creating an additional burden for the future readers looking for a solution for the topic. In that sense, you are creating a noise if you are creating a redundant article. So what we should do when we found that the topic that we would like to wrtie is available. Check the original post and if you have an additional content, just update the original post. If you are sure that you can write a better one by rewriting, replace the original post. The original post should be archived to the old technical note and referenced by the new article.\nWe share the open source code and how-to manual as a final output. We use GitHub as the main archive As a rule of thumb, I expect 50% of everyone’s time will be spent writing what you have done.\nHow to write a journal  Make the outline neat If you click the View-\u0026gt; Show Document Outline button, it will show you the document outline. Make sure your writing follows the header rule. There is a shortcut key for selecting the paragraph level.  cmd + alt + 1 makes the top level heading cmd + alt + 2 makes the second level heading cmd + alt + 0 makes the content paragraph, which is not shown in outline.   The Things to do next part is important. Your professor reads it often. Keep it tidy. When something is done, remove. Update it everyday. When you are given new goal, write it there. Make the thing you are working on now, the first item. Order them in the priority.  Communication is half of your job. Your impact will be the multiplication of research and communication of the result. Try to report the progress whenever there is a progress Don\u0026rsquo;t wait until your boss ask your progress. Tell him what you are working on now. If you meet a barrier or decide to change direction, report it. When I ask you to do something, I usually expect to hear the progress report in a one or two days.\nBeing true to ourselves You cannot deceive yourself. Being true to yourself is, in my opinion, a very good long-term strategy. For example, if you don’t know during the seminar, acknowledge that you don’t know and ask questions until you understand. Don’t be embarrassed or give up by social pressure. Academy is a few places where being true to yourself is tolerated.\nOther good question to ask yourself is whether you are enjoying the research. Actually according to my experience, research is not for everyone. Probably about 5 persons out of 100 suit the academic life. Because of this, it is perfectly okay if you feel that the graduate study is not what you want to do. I recommend to get a SW engineering job in that case, which actually pays you better and provides better work-life balance. You might feel like a failure to quit the research at first, but if you are true to yourself, you may find it as one of the best decisions of your life later.\nSimilarly I will release you out of the lab if I think that you are not a good fit for the lab as early as possible. How I make that decision? First, I have to acknowledge that it is very difficult decision for me, too. It is not only emotionally difficult but intellectually difficult. Because nobody knows the future, one day the student I fired may get a Nobel prize.\nI have to follow the lab culture which is my principle. For me, there are three characteristics for the academic career.\n First is curiosity. The desire to know or find the truth is our strongest motivation and rewards.\n Second is the self-starting. This one is related to curiosity. Because you want to know something, you start researching about it. This is usually observed when the student comes to me and ask an advice for something. It is the opposite of waiting for the order from me.\n Third is the persistence. The journey is long and the rewards are sparse. If you are not persistent, chances are you give up. Persistence is measured by the shareable output. According to my experience, quantity is more important than the quality for measuring persistence. It means if you are producing lots of so-so blog article or technical note and low quality draft of academic papers, you have the persistence. Think about growing academic muscle by writing everyday.\n  ","date":1565067600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"3f80f58bdfbdbbcdacb6f39ae8597a48","permalink":"http://crystal.uta.edu/~park/post/hdilab-culture/","publishdate":"2019-08-06T00:00:00-05:00","relpermalink":"/post/hdilab-culture/","section":"post","summary":"In HDILab, we learn by teaching. Do not study or work but create a sharable output.  Be true to yourself.  ","tags":["student advice"],"title":"The HDILab culture","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Life evolves. The speed of evolution depends on the mechanism of the evolution. The first evolution was based on the random mutation of genes. It is basically trial and error. But the diversity of the life forms it generated is awesome. I would call it hard evolution. We, humans, are one of the successful life forms with mosquitoes and cockroaches.\nHuman adopted a new strategy. It was building a more flexible general purpose brain to adapt to the environment. Other life forms also have the brain. But its function was more rigid or hard encoded. You can think how the pocket calculator compares with the smartphone. Both have a similar electronic components namely CPU, memory and I/O devices.\nUsing the general brain, we can do many things that was not possible. But the essential function is learning. In hard evolution, the special variation that worked well leaves the genes. However, in soft evolution, we can learn from others. It basically allows exponential trial and errors. Nobody can experience all the diverse experience in the world given limited time and energy. Still we can learn from others first using verbal language. This allowed us to transfer knowledge from nearby people. The written language allowed it to go beyond the limitation of the space and time of the verbal language. Now what is beautiful about the soft evolution is that the time it takes to evolve has reduced dramatically. The soft evolution happens in the brain circuitry during the life time of the agent while in the hard evolution, the life time of the agent was a single unit of progress.\nBecause of this general flexible brain, we are born with a rather blank state compared to other animals such as cats and dogs. (Of course there are still many innate circuitry which I will talk later about.) And it takes relatively long time to have a reasonably working program. Interestingly as the amount of the information to function well in the society increases the time it takes to be independent is also increasing. Now it is common in the first world that it takes about 20 to 30 years before one can be independent from their parents.\nI said the brain is born as a basically blank state. But still there are innate mechanism to help learn from the parents. Think of the truly blank state agent. We got a continuous stream of image and sound. How we can make sense of the these images? For example, parents talking to the baby might look like a skin colored chunk with a few holes that are constantly moving. One hole continuously changes shape and make a sound (hint: mouth). How can we learn this chunk of image is a mom and the sound is the language which is different from random noise from the street? Epistemology is an old branch of philosophy devoted to this problem.\nI think the innate mechanism is essential for the new born baby to make sense of the world. Let me give you an example. Andrew Meltzoff and Keith Moore found that a baby who was about 7 hours since the birth can still mimic the tongue movements. It is quite surprising when we think about all the skills to do this. First, the baby has to recognize the human face. Then he has to map the human face he never seen to his face that he never seen himself. Finally the baby has to control the tongue to imitate the expression.\n New born baby can imitate the random facial expression. This capabilities an innate mechanism to help learn from others. We should learn from relevant objects such as people and care givers and not from irrelevant objects such as cars or doors. Image from Science 1977   I believe this imitation capability is essential to learn from others. Given blank state, we should not fill this blank with random noise but relevant contents. What will be the relevant contents for the new born baby. Most likely it will be from the parents and caregivers. The imitation mechanism has already filtered irrelevant objects in the world and focused on the human facial expression and followed its motion. This limits the vast space of the exploration to meaningful trajectory that was suggested by human.\nLessons for the AGI In the famous DQN paper, there are experiments for the Atari games. Among many games that showed superhuman performance by the artificial agent, there was one game that showed very poor performance called Montezuma\u0026rsquo;s revenge.\n  This blog explains why the Montezuma\u0026rsquo;s revenge game is particularly challenging. In a summary, there are too many things that have to be done sequentially such as getting the key, avoid the monster, and go back to the door. The random behavior by the agent cannot explore the vast exploration space in a limited time. In 100 Million iterations, the agent can access only the second stage. This shows the limitation of the random trial and error. I said other animals cannot learn. But I was wrong. They can learn. For example, dogs can learn new trick. They learn by trial and error. They associate behaviors with positive rewards or treats. But things you can learn from this approach is limited.\nI think there are three components for the artificial general intelligence. The first is a model. For example, in my HPM theory, the agent predicts the next sensory input with the help from the hierarchy to do the higher-order prediction (chunking). The second is an environment. It provides a source of information. The problem is there are vast space for exploration. Finally there will be many smart tricks such as imitation or social mechanism to focus on the human interaction during language acquisition to reduce this search space effectively by social learning. Our lab tries to build an environment that can provide a testbed for such social mechanism and study the models.\nFinally, what will be the next step for the evolution? It will be how we can overcome the limitation of the biological brain cell. The brain cell is relatively slow and degradable. The number of cells are limited by how large our head can be to enable the birth and feed required energy to run big brain. If we can use our electronic components as a device, it will achieve a step-up in the evolution speed. But more importantly, I think the true power of the next evolution is when the artificial intelligence agent upgrades its own structure. This upgrades will be beyond the human\u0026rsquo;s ability to follow which is called technical singularity. Some would call it computational evolution.\n The evolution of the intelligence   ","date":1564981200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565117378,"objectID":"9e80d9eba7c40220c7be82fbc83fd54e","permalink":"http://crystal.uta.edu/~park/post/soft-evolution/","publishdate":"2019-08-05T00:00:00-05:00","relpermalink":"/post/soft-evolution/","section":"post","summary":"The first evolution mechanism was based on the DNA. The second evolution mechanism was based on the brain. The third  evolution mechanism will be based on algorithms","tags":["agi","research"],"title":"Evolution: Hard, Soft, and Computational?","type":"post"},{"authors":["Deokgun Park"],"categories":null,"content":" Contents  Introduce the role of the rewards in reinforcement learning  Use the example of dog training Explain the dopamine Explain the  Introduce an extrinsic rewards  Classic rewards Used in reinforcement learning examples  Atari Alpha Go World model paper   Introduce an intrinsic rewards\n Explain the limitation of the extrinsic rewards  Montezuma\u0026rsquo;s revenge  Explain the mechanism of the intrinsic rewards  How the curious or explorative behavior relates to the intrinsic rewards How to formulate the intrinsic rewards mathematically?  Model-free vs model-based reinforcement learning Intrinsic rewards examples  Introduce paper by Stanford World Discovery Model paper   Introduce the limitation of the intrinsic rewards\n The search space is too vast for a single agent learn by exploration\n Example: Radnom permutation of alphabet to generate a meaningful sequenc. Example: Musical note\n  Explain the higher order Markov chains\n   Introduce the social rewards\n Introduce the imprint Introduce how the baby learns  Learn by imitation Joint attention Gated learning  Theory based mathematically driven algorithm vs quick dirty rule based system  Future research direction\n Models that take into account the innate mechanisms Environments that can test the models with social rewards   Reward plays a key role in the reinforcement learning.\nIn this post, I will explain three types of the rewards that can shape the intelligent behavior.\n Extrinsic rewards: A reward triggered by the external entity. Intrinsic rewards: A reward that can be generated by the agent himself. Social rewards: A rewards that can be generated by the social activities  Extrinsic rewards Extrinsic rewards means the positive or negative reward triggered by the external entity. A concrete example is the Previously rewards meant mostly extrinsic rewards.\nIntrinsic rewards Social rewards A search space to generate a meaningful space is very vast and only very few are meaningful. But we can learn from others the meaningful sequence which reduces the search space\nThis can be expressed with music note.\nShould we use reward mechanism or reflex mechanism?\nImprint is an example of reflex mechanism. Because we have the freedom to follow or not, I think reward mechanism makes more sense.\nReferences  Jones, Susan S. (2012-12-10). \u0026ldquo;Human Toddlers\u0026rsquo; Attempts to Match Two Simple Behaviors Provide No Evidence for an Inherited, Dedicated Imitation Mechanism\u0026rdquo;. PLOS ONE. 7 (12): e51326. Bibcode:2012PLoSO\u0026hellip;751326J. doi:10.1371/journal.pone.0051326. ISSN 1932-6203. PMC 3519587. PMID 23251500 Jones, Susan S. (2009-08-27). \u0026ldquo;The development of imitation in infancy\u0026rdquo;. Philosophical Transactions of the Royal Society B: Biological Sciences. 364 (1528): 2325–2335. doi:10.1098/rstb.2009.0045. ISSN 0962-8436. PMC 2865075. PMID 19620104. Rowland, D.C., Yanovich, Y. and Kentros, C.G. (2011). A stable hippocampal representation of a space requires its direct experience. Proceedings of the National Academy of Sciences. 108(35). 14654-14658. -\u0026gt; An evidence for Gated language CONSPEC and CONLERN: a two-process theory of infant face recognition. J Morton, MH Johnson - Psychological review, 1991 - psycnet.apa.org Evidence from newborns leads to the conclusion that infants are born with some information about the structure of faces. This structural information, termed CONSPEC, guides the preference for facelike patterns found in newborn infants. CONSPEC is contrasted with a\n Newborns\u0026rsquo; preferential tracking of face-like stimuli and its subsequent decline MH Johnson, S Dziurawiec, H Ellis, J Morton - Cognition, 1991 - Elsevier Abstract Goren, Sarty, and Wu (1975) claimed that newborn infants will follow a slowly moving schematic face stimulus with their head and eyes further than they will folow scrambled faces or blank stimuli. Despite the far-reaching theoretical importance of this … How the brain processes social information: searching for the social brain TR Insel, RD Fernald - Annu. Rev. Neurosci., 2004 - annualreviews.org ▪ Abstract Because information about gender, kin, and social status are essential for reproduction and survival, it seems likely that specialized neural mechanisms have evolved to process social information. This review describes recent studies of four aspects of social Eye contact detection in humans from birth T Farroni, G Csibra, F Simion… - Proceedings of the …, 2002 - National Acad Sciences Making eye contact is the most powerful mode of establishing a communicative link between humans. During their first year of life, infants learn rapidly that the looking behaviors of others conveys significant information. Two experiments were carried out to demonstrate …  ","date":1564981200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"31334378b8fe8152420ce9ac4e6886cf","permalink":"http://crystal.uta.edu/~park/post/rewards/","publishdate":"2019-08-05T00:00:00-05:00","relpermalink":"/post/rewards/","section":"post","summary":"Extrinsic rewards is for the animal or bug level of intelligence. Intrinsic rewards or curiosity enables efficient navigation in the vast exploration space. Social rewards enable imitation or gated learning which reduces the exploration space.  ","tags":["agi","research"],"title":"Rewards for AGI","type":"post"},{"authors":["Deokgun Park","Steven M. Drucker","Roland Fernandez","Niklas Elmqvist"],"categories":null,"content":"  Sequence of layout operations to generate a unit column chart for survivors of the Titanic by passenger class.   ","date":1538016791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"f2aba7552741a49797bb1b7afbffe7d3","permalink":"http://crystal.uta.edu/~park/publication/atom/","publishdate":"2018-09-26T21:53:11-05:00","relpermalink":"/publication/atom/","section":"publication","summary":"Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark - a visual unit - during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.","tags":["Information Visualization"],"title":"Atom: A Grammar for Unit Visualizations","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536469200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"http://crystal.uta.edu/~park/tutorial/example/","publishdate":"2018-09-09T00:00:00-05:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Deokgun Park","Seungyeon Kim","Jurim Lee","Jaegul Choo","Nicholas Diakopoulos","Niklas Elmqvist"],"categories":null,"content":"  The CommentIQ UI showing toggleable visualizations such as scatterplot, map, and timeline (left) that enable overview and filtering of comments, as well as an adjustable ranking based on various weighted quality criteria (right).ConceptVector supports interactive construction of lexicon-based concepts. Here the user creates a new unipolar concept (1) by adding initial keywords related to tidal flooding (2). The system recommends related words along with their semantic groupings (3), also shown in a scatterplot (4), revealing word- and cluster-level relationships. Irrelevant words can be specified to improve recommendation quality (5). Concepts (9) can then be used to rank document corpora (10). Document scores can be visualized in a scatterplot based on concepts such as tidal flooding and money (7). Users can further refine concepts based on results (8).   ","date":1517195878,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"b66bafc1a63aa7d16637aa7c7c4931de","permalink":"http://crystal.uta.edu/~park/publication/conceptvector/","publishdate":"2018-01-28T22:17:58-05:00","relpermalink":"/publication/conceptvector/","section":"publication","summary":"Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.","tags":["Text Analysis","Visual Analytics","Open-ended Tasks"],"title":"ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding","type":"publication"},{"authors":["Deokgun Park"],"categories":null,"content":"","date":1483250400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548263775,"objectID":"b114db2e37f3222e6421fb54941f18d1","permalink":"http://crystal.uta.edu/~park/talk/naver/","publishdate":"2017-01-01T00:00:00-06:00","relpermalink":"/talk/naver/","section":"talk","summary":"A comment section, such as for an online news article, video, or social media post, is commonly a place to share personal opinions. However, when there are too many comments, it is challenging to gain new insights and estimate the distribution of the public opinions simply by reading them. Selecting and promoting high-quality comments can mitigate those problems, but the required resources for choosing them manually are too high for common practice. In this talk, I will introduce my research on visual analytics for comment analysis. The ability to see an overview of the comments and to create custom rankings supports moderators, editors, and commenters themselves. I also propose a method to aid semantic analysis where users can build custom dictionaries for unique concepts and use them to analyze comments. In the future, assessing public opinions will play an important role in protecting the digital democracy against organized attempts to manipulate public opinions.","tags":[],"title":"Visual Analytics for Comment Analysis (in Korean)","type":"talk"},{"authors":["Deokgun Park","Simranjit Sachar","Nicholas Diakopoulos","Niklas Elmqvist"],"categories":null,"content":"  The CommentIQ UI showing toggleable visualizations such as scatterplot, map, and timeline (left) that enable overview and filtering of comments, as well as an adjustable ranking based on various weighted quality criteria (right).   ","date":1474950413,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"f7a8d1ac5322c7c1c36cc05d2b9bf386","permalink":"http://crystal.uta.edu/~park/publication/commentiq/","publishdate":"2016-09-26T23:26:53-05:00","relpermalink":"/publication/commentiq/","section":"publication","summary":"Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.","tags":["Text Analysis","Visual Analytics","Open-ended Tasks"],"title":"Supporting comment moderators in identifying high quality online news comments","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"http://crystal.uta.edu/~park/project/deep-learning/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":["Deep Learning"],"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"http://crystal.uta.edu/~park/project/example-external-project/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Deokgun Park"],"categories":null,"content":" Hello,\nMy name is Deokgun Park and I lead Human Data Interaction Lab (HDILab) in the Department of Computer Science \u0026amp; Engineering at the University of Texas at Arlington. In this article, I will explain the problems we are solving at HDILab. In Human Data Interaction Lab, we are studying artificial general intelligence (AGI).\nMany students already have an idea about artificial intelligence (AI). But artificial general intelligence or AGI might be a new term. So let\u0026rsquo;s start with what AGI is.\nClarifying AI and AGI Since Marvin Minsky and other researchers gathered at Dartmouth College in Hanover in 1956, the goal of AI was to build a machine that can do many tasks like humans. But after experiencing a few AI winters, the academic focus shifted to building models to perform a single application because it was more tractable. This approach has been frequently called machine learning (ML). It is said that one of the reason ML has been coined was to avoid mentioning AI in the proposal during the AI winters. It was successful, and we achieved many advances in specific applications such as image classification, machine translation, self-driving car, or playing Go game. And the term AI became hot again.\nBut we still lack an idea about how we can build a general-purpose AI which appears in Hollywood movies or general public associates with. Because most of the current AI research is application-specific, we need another term for the original general purpose agent. It has been rephrased many times, including\n strong AI true AI human-like AI movie-like AI lifelong learning continual learning meta learning or learning to learn artificial general intelligence (AGI)  We will use AGI to refer this original AI because it tells the fundamental difference between the current mainstream AI research, which is application-specific.\n AI vs AGI. AI started as a general purpose model, but later changed its meaning to mostly application specific models. We will use artificial general intelligence (AGI) to mean the original general purpose model.   How to test AGI Before we discuss how we can build AGI, let\u0026rsquo;s start with how we can test if we built AGI. Because according to Peter Drucker or Lord Kelvin,\n If you can\u0026rsquo;t measure it, you can\u0026rsquo;t improve it.\n Alan Turing suggested a test to verify AGI. According to the Turing test, also known as Imitation Game, a human participant asks an agent hidden behind the wall to perform many tasks. If the human cannot discern whether the agent is truly a human or an artificial agent, then the artificial agent is assumed to achieve a human-level intelligence.\n In the movie Ex Machina, you can see how Turing test is conducted.   While theoretically valid, it poses several problems in its practical application. First, it is too difficult for the current status of research. The human tester can use all the knowledge about the world to test the agent, yet it would be very costly and impossible to teach all the knowledge about the world to the artificial agent, while researchers struggle to discover how we can mimic human-like learning. It is like asking a 1st grader to take SAT test.\nSecond, the test does not provide any idea about where the model can learn those knowledges required to take test. It is like testing students without giving them any textbooks or lectures.\nThird, even for the same model and the same evaluator, the feedback will be different from today to tomorrow. The test is subjective and not reproducible.\nFinally, it is prohibitively expensive to hire people to conduct a test. Humans may participate in annual Turing test competition, but during the hyperparameter tuning or iterative model refinement, it is difficult to get access to human testers. For these reasons, it is not practically applicable in the current renaissance of the AI.\nWe propose an alternative test method for AGI. This method is based on the observation that even if we raise cats and dogs, treating them like human babies, they cannot learn to speak. The animals are capability-limited, and the human baby cannot learn to speak if it is separated from people speaking to it. It is environment-limited. Therefore we can think that the language acquisition is the function of the environment and the capability of the learning agent. In other words, if an agent can learn how to speak in a proper environment, we can say that it has the capability to do artificial general intelligence. We call it the Language Acquisition Test for AGI or Park\u0026rsquo;s test.\n In Park\u0026rsquo;s test, the agent is said to have a human-level intelligence if it can learn language given the proper environment   To pass a Park\u0026rsquo;s test, we needs a capable model and a proper environment. Let\u0026rsquo;s look at environment first.\nThe Environment for the Language Acquisition To conduct the Park\u0026rsquo;s AGI Test, an agent requires an interaction with the world to learn how to speak. One factor that enabled the recent advances in reinforcement learning was the use of the simulated environments such as Atari games or 3D first person shooting games such as VizDoom. Those environments can be used to build and test an agent that can optimize its behavior with little instructions while maximizing the reward signal as a goal to train an agent. However, environments and reward signals adopted in those studies are primitive and more suitable for the development of low-level intelligence which can be found in fish or bugs.\n               Atari Environment Car Racing Environment RoboSchool Environment    If we want to build an agent with human-level of intelligence, we need to use an environment that can provide a reasonable sensory input and feedback that can teach an agent how to speak. One naive way to provide such an environment is using real people to provide the required responses or trainings. However, using real people has similar limitations with the Turing test. We will have to explore many models in a trial and error before we can finally build an AGI. In addition to the prohibitive cost of using human participants, human experimenters will become quickly tired of providing the same feedback again and again or providing different feedback to the same situation which does not lead to reproducible research.\n   Advanced Discussion: What is language?     Some would claim that the poor baby abandoned in jungle will still develop a language to communicate with other animal. Similarly there is an emergent communication pattern among collaborative robots. While true, we are interested in human language in this project especially for human robot interaction. The rationale is that we want the robot to learn human language not vice versa.    There are a few prior researches in language acquisition. Devendra Singh Chaplot and other researchers in the Carnegie Melon University used VizDoom environment to demonstrate how agent can learn semantic concepts using reinforcement learning. In their experiments, agents get rewards when they go to objects according to the verbal direction such as \u0026ldquo;Go to short blue torch\u0026rdquo;. What is interesting is that during the training the agents experience verbal direction such as \u0026ldquo;Go to blue torch\u0026rdquo;, \u0026ldquo;Go to torch\u0026rdquo;, \u0026ldquo;Got to short red torch\u0026rdquo; and so on but never experience \u0026ldquo;short blue torch\u0026rdquo;. Even then during the test time, they can follow the direction to go to \u0026ldquo;short blue torch\u0026rdquo;. This implies that the agent can map the words such as \u0026ldquo;short\u0026rdquo;, \u0026ldquo;blue\u0026rdquo;, \u0026ldquo;torch\u0026rdquo; with the visual features in the objects. This is a step forward to solve the symbol grounding problem.\n Agents learns language using reinforcement learning. Image by Devendra Singh Chaplot et al   But the concepts that can be learned in this way is limited. There are limitations due to the environments. We don\u0026rsquo;t learn by fetching objects according to the parent directions. It will take too long to learn all concepts this way. We need a better environments that can teach the language\nWhen the transistor was first invented, people were excited to use it as an amplifier to build radio and radars. But the true power of the transistor was when it was arranged in a specific structure, it could build a logic gate. By arranging logic gates in a specific way, we could build a CPU, the true ultimate power of transistor.\nThe same goes with the connectionism. We know how one or two neural cell behaves exactly and we can simulate them. But most advancement came with the arrangement of this device in a specific ways, such as multi-layer perceptron (MLP), convolutional neural net (CNN), recurrent neural net (RNN), and generative adversarial network (GAN). In this sense, we are experimental computer scientist who seek the right structure mostly by trial and errors. Because of this nature, it is helpful to have an environment that can provide an easy, low-cost, and reproducible way to experiment.\nIn our lab, we would like to develop an environment for Park\u0026rsquo;s test. The key idea is that we will focus on the critical stage of the human development when humans learn how to speak as infants. This environment will contain a 3D replication of a room in the home with a few toys. There will be a mother character and a baby character. The mother character will be programmed manually using traditional game AI technology to take care of the baby and lead a conversation with the baby character which is often called a baby talk, child-directed speech (CDS), motherese or infant directed speech (IDS). IDS is a communication pattern, when a mother tries to teach language to an infant. The baby character will be the learning agent. The success of the test will be decided by whether the learning agents can acquire language and develop reasonable behavior comparable to the human developmental progress.\n Previous environments have led the advancements of the reinforcement learning. We propose a novel environment for the language learning.   Hierarchical Prediction Memory (HPM) HDILab also studies the model that can learn the language in the environment described above. Let\u0026rsquo;s start our discussion from the function and the mechanism.\nFunction and Mechanism The most difficult way to fly is to imitate the biological mechanism of flying such as in birds or flies. The biological mechanism is highly evolved due to the long history of adaptation. And many of the complex mechanism is due to the biological limitation which does not apply to us. What we need to figure out is what those mechanism are actually doing or function. The way human or horses run is pushing the ground backward. The way birds fly is pushing the air downward. This applies to the intelligence. The biological mechanism is very complex. It has been very optimized. It has to use biological lossy devices. So the mechanism is very complex. But we need to learn what is the function of the mechanism. It is in my opinion prediction of vector sequence. Hierarchy helps to overcome the temporal limit by chunking. The essence of intelligence is hierarchical prediction of vector sequence.\nWe conjecture that the essence of the intelligence is a hierarchical prediction.\nWhere the original connectivist has missed? The original connectivist thought the decision or pattern recognition as the core activity. However, this lead to the wrong formalization of the problem? In my opinion, prediction should be the core activity. Prediction includes the decision or pattern recognition, but the main difference is that it has the streaming input and streaming output.\n","date":1461128400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568217054,"objectID":"59de895a25b9e24e0fe72f00b1f2227a","permalink":"http://crystal.uta.edu/~park/post/research-in-hdilab/","publishdate":"2016-04-20T00:00:00-05:00","relpermalink":"/post/research-in-hdilab/","section":"post","summary":"Artificial General Intelligence (AGI) Research at Human Data Interaction Lab (HDILab)","tags":["agi","research","student advice"],"title":"AGI Research in HDILab","type":"post"},{"authors":["Deokgun Park","Jungu Choi","Niklas Elmqvist"],"categories":null,"content":"  The MovieVis tool. Two groups in the movie space have been selected to compare corresponding user distribution. Two movies selected in the upper-center region—One flew Over the Cuckoos Nest (1975) and Amadeus (1984)–and are shown in blue color. Another two movies selected in a lower-center region—Phenomenon (1996) and Twister (1996)—are shown in orange. The highlighted users are those who liked all both pairs of movies (because the group mode is set to common). Based on the user space axes—gender for the horizontal and age for the vertical—we can see that while the movie One Flew Over the Cuckoo’s Nest and Amadeus were favored by male reviewers of all ages, the Phenomenon and Twister were liked by relatively younger male audiences.    On the left, we compare two movies, Toy Story (1995), in blue, and Scream (1996), in orange, according to the age, location and similarity criteria for users. Some notable observations are while the former is liked all around the U.S. by any age groups the latter is mostly popular in the eastern part and within a younger generation. On the right, we compare two users, a 19-year-old male student, in blue, and a 51-year-old male educator, in orange according to the average, release date, and similarity criteria for movies. We observe that the older user tends to rate older films highly. In addition, his average review tends to conform to the average ratings patterns of all users while the younger user seems to deviate from it.   ","date":1459266011,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538237872,"objectID":"6dfd7923368efd00696a61be7c219db1","permalink":"http://crystal.uta.edu/~park/publication/parallelspace/","publishdate":"2016-03-29T10:40:11-05:00","relpermalink":"/publication/parallelspace/","section":"publication","summary":"We present ParallelSpaces, a novel method to explore bipartite datasets in both feature and data dimensions. This dyadic data is displayed as weighted bipartite graphs using scatterplots in two separated visual spaces, where each entity is positioned according to multi-dimensional properties of each entity or similarity in preferences. Selecting or navigating in one space is reflected in the other space, so that organic visual patterns can be formed to facilitate the characterization of underlying groupings. To aid visual pattern recognition we also overlay a contour plot based on kernel density estimation. We have implemented two instantiations of ParallelSpaces for (a) movie preferences, and (b) business reviews as Web-based visualizations. To validate the method, we performed a qualitative user study involving eleven participants using these Web-based tools to explore data and collect deep insights. ","tags":["Information Visualization","Visual Analytics"],"title":"Parallelspaces: Simultaneous Exploration of Feature and Data for Hypothesis Generation","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538188338,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"http://crystal.uta.edu/~park/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]